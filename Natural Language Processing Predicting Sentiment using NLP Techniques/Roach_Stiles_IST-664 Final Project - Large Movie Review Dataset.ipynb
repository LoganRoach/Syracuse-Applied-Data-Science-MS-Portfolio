{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5445f67e",
   "metadata": {},
   "source": [
    "# IST 664 - Natural Language Processing\n",
    "**Mark Stiles | Logan Roach**<br>\n",
    "**March 3, 2024**<br>\n",
    "**Final Project: Stanford's Large Movie Review Dataset**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc904a7",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0888a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.collocations import *\n",
    "from bs4 import BeautifulSoup\n",
    "import pathlib as pathlib\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b580c",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data is sourced from a Stanford's large movie review dataset. There are 25,000 reviews. Half are classified as positive and the other half negative. Each review is an open text block written about an unnamed movie and was reviewed and annoted as a research project. \n",
    "\n",
    "**URL: https://ai.stanford.edu/~amaas/data/sentiment/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaac08f",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Much of the work is reusable and is contained in the following list of functions that were developed over several iterations. They were separated into three groups: Data Prep, Feature Generation and Training functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565ded1",
   "metadata": {},
   "source": [
    "### Data Prep Functions\n",
    "\n",
    "**build_review_df**\n",
    "\n",
    "The build_review_df function takes in a list of file paths and a class value (sentiment value). This is the initial conversion of data files to a dataset. It will load each of the files and build a dataframe. There will be two dataframes, one with positive and one with negative classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2114247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_review_df(files, class_value):\n",
    "    \n",
    "    # build a list of the provided class value for each file in the list\n",
    "    all_classes = [class_value for pf in pos_files]\n",
    "    all_texts = []\n",
    "    \n",
    "    # loop through all files and get the text\n",
    "    pos = 1\n",
    "    for file_path in files:\n",
    "        \n",
    "        # read out contents\n",
    "        f = open(file_path, \"r\", encoding=\"latin-1\")\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "\n",
    "        # strip out tags\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        for data in soup(['br']):\n",
    "            data.decompose()\n",
    "        text = ' '.join(soup.stripped_strings)\n",
    "                \n",
    "        all_texts.append(text)\n",
    "\n",
    "        pos += 1\n",
    "    \n",
    "    # create dataframe with the class and text lists\n",
    "    new_df = pd.DataFrame({'sentiment': all_classes, 'review': all_texts})\n",
    "    \n",
    "    print(f'built review df for class: {class_value} with dimensions: {new_df.shape}')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa2a16",
   "metadata": {},
   "source": [
    "**get_movie_review_text**\n",
    "\n",
    "The get_movie_review_text function will find all the file names by class type and then use build_review_df to create two dataframes that will then be merged into one dataframe and stored as a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab107e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_review_text():                \n",
    "    \n",
    "    # load files from local directory\n",
    "    prefix = \"./LargeMovieReviewData/train/\"\n",
    "    pos_files = [f for f in pathlib.Path(prefix + \"pos\").iterdir()]\n",
    "    neg_files = [f for f in pathlib.Path(prefix + \"neg\").iterdir()]\n",
    "    \n",
    "    # create a dataframe for each list of classification files\n",
    "    pos_df = build_review_df(pos_files,\"pos\")\n",
    "    neg_df = build_review_df(neg_files,\"neg\")\n",
    "\n",
    "    # merge the dataframes with positive and negative classes\n",
    "    merge_df = pd.concat([pos_df, neg_df], axis=0)\n",
    "    \n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7702ff",
   "metadata": {},
   "source": [
    "**tokenize_and_remove_stopwords**\n",
    "\n",
    "The tokenize_and_remove_stopwords function is used by the bag of words feature functions to get a trimmed set of words by removing stop words and removing a configurable set of words by parts-of-speech category. Additional symbols representing unknown parts-of-speech will also be removed to help create a more compact and meaningful bag-of-words list. This uses the Penn-Treebank parts-of-speech list. \n",
    "\n",
    "**Source: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cae685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(word_string, pos_keys):\n",
    "    \n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    new_list = []\n",
    "        \n",
    "    # get pos tags for all words\n",
    "    tagged_words = nltk.pos_tag(word_list)\n",
    "        \n",
    "    # store characters and parts of speech to be removed\n",
    "    skip_values = ['#','as','s','ed','ii','al','t','\\'the','%','\\'m', '\\'', '\\'s','A','n\\'t','do','\\'re','\\'ve','*','i','I','ca','\\'ll','\\'d','..','....','wo','Â','$','de','b','la']\n",
    "    \n",
    "    # dictionary of parts-of-speech groupings\n",
    "    pos_dict = {\n",
    "        'conj' : ['CC','IN'],\n",
    "        'determ' : ['DT','PDT','WDT'],\n",
    "        'noun' : ['NN','NNS','NNP','NNPS','PRP','PRP$','WP','WP$'],\n",
    "        'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ','MD'],\n",
    "        'adj' : ['JJ','JJR','JJS'],\n",
    "        'adv' : ['RB','RBR','RBS','WRB'],\n",
    "        'particle' : ['RP'],\n",
    "        'marker' : ['LS'],\n",
    "        'num' : ['CD','LS'],\n",
    "        'foreign_word' : ['FW'],\n",
    "        'symbol' : ['SYM'],\n",
    "        'interj' : ['UH'],\n",
    "        'to' : ['TO'],\n",
    "        'ex' : ['EX'],\n",
    "        'pos' : ['POS']\n",
    "    }\n",
    "    \n",
    "    # gather all the requested pos tags into one list (with additional symbols)\n",
    "    skip_tags = ['(',')',':','``',\"''\",'.',',']\n",
    "    for key in pos_keys:\n",
    "        skip_tags += pos_dict[key]\n",
    "    \n",
    "    # get nltk base stopwords\n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english') \n",
    "    custom_stop_words = ['the', 'it', 'this', 'and', 'in'] + nltk_stop_words\n",
    "    \n",
    "    # filter out characters, POS tags and stop words\n",
    "    filtered_list = []\n",
    "    for (word, tag) in tagged_words:\n",
    "        if(tag not in skip_tags and word not in skip_values and word not in custom_stop_words):\n",
    "            filtered_list.append((word,tag))\n",
    "    \n",
    "    # extract only the remaining words and set them to lowercase\n",
    "    word_list = [word.lower() for (word, tag) in filtered_list]\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9b9d8",
   "metadata": {},
   "source": [
    "### Feature Generation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20c588",
   "metadata": {},
   "source": [
    "**Bag-of-Words**\n",
    "\n",
    "**sub_bow_features and generate_bow_features**\n",
    "\n",
    "The sub_bow_features and generate_bow_features work together to build a bag-of-words feature set for all the reviews. The generate function takes care of the initial tokenization and word frequency distribution so it's not repeatedly being created and then iterates through each review while calling the sub function. The sub function then builds the features for each individual review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01aed203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_bow_features(word_string, word_features):\n",
    "    \n",
    "    lower_word_string = word_string.lower()\n",
    "\n",
    "    features = {}\n",
    "    \n",
    "    # top word features\n",
    "    for word in word_features:\n",
    "        features[f'V_{word}'] = (word.lower() in lower_word_string)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def generate_bow_features(pos_categories, feature_count):\n",
    "    \n",
    "    tokenized_reviews = [tokenize_and_remove_stopwords(row.review,pos_categories) for (pos,row) in reviews_df.iterrows()]\n",
    "    all_words_list = [word for review in tokenized_reviews for word in review]\n",
    "    word_dist = nltk.FreqDist(all_words_list)\n",
    "    top_ranked_words = word_dist.most_common(feature_count)\n",
    "    top_words = [word for (word,count) in top_ranked_words]\n",
    "    feature_sets = [(sub_bow_features(row.review, top_words), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3974ad5",
   "metadata": {},
   "source": [
    "**Parts-of-Speech**\n",
    "\n",
    "**count_tags**\n",
    "\n",
    "The count_tags function will take a list of tags and sum all the counts for each tag in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27b7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(word_dist, tag_list):\n",
    "    \n",
    "    # sum the usage counts for each parts-of-speech tag in the list\n",
    "    tag_count = sum(word_dist[tag] for tag in tag_list)\n",
    "    \n",
    "    return tag_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52369f54",
   "metadata": {},
   "source": [
    "**sub_pos_features and generate_pos_features**\n",
    "\n",
    "The sub_pos_features and generate_pos_features functions work together to build a parts-of-speech feature set for all the reviews. The generate function iterates through each review while calling the sub function. The sub function first tokenizes and tags the review text with parts-of-speech and then builds a frequency distribution of the tags to then pass to the count_tags function to build the features for each individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd261e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_pos_features(word_string):\n",
    "    \n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    tags = [tag for (word,tag) in nltk.pos_tag(word_list)]\n",
    "    word_dist = nltk.FreqDist(tags)\n",
    "\n",
    "    features = {}\n",
    "    \n",
    "    # pos counts\n",
    "    features['V_conj_count'] = count_tags(word_dist, ['CC','IN'])\n",
    "    features['V_determ_count'] = count_tags(word_dist, ['DT', 'PDT', 'WDT'])\n",
    "    features['V_noun_count'] = count_tags(word_dist, ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'PRP$', 'WP', 'WP$'])\n",
    "    features['V_verb_count'] = count_tags(word_dist, ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD'])\n",
    "    features['V_adj_count'] = count_tags(word_dist, ['JJ', 'JJR', 'JJS'])\n",
    "    features['V_adv_count'] = count_tags(word_dist, ['RB', 'RBR', 'RBS', 'WRB'])\n",
    "    features['V_particle_count'] = count_tags(word_dist, ['RP'])\n",
    "    features['V_marker_count'] = count_tags(word_dist, ['LS'])\n",
    "    features['V_num_count'] = count_tags(word_dist, ['CD', 'LS'])\n",
    "    features['V_foreign_word_count'] = count_tags(word_dist, ['FW'])\n",
    "    features['V_symbol_count'] = count_tags(word_dist, ['SYM'])\n",
    "    features['V_interj_count'] = count_tags(word_dist, ['UH'])\n",
    "        \n",
    "    return features\n",
    "\n",
    "def generate_pos_features():\n",
    "    \n",
    "    feature_sets = [(sub_pos_features(row.review), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a1339",
   "metadata": {},
   "source": [
    "**Text Statistics**\n",
    "\n",
    "**sub_text_stat_features and generate_text_stat_features**\n",
    "\n",
    "The sub_text_stat_features and generate_text_stat_features functions work together to build a feature set for all the reviews based on basic statistics like the count of capital letters or the length of the review. The generate function iterates through each review while calling the sub function. The sub function first tokenizes the review text and then calculates basic statistics to build the features for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c906ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_text_stat_features(word_string):\n",
    "    \n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # text statistics\n",
    "    features['V_cap_count'] = sum(1 for c in word_string if c.isupper())\n",
    "    features['V_review_length'] = len(word_string)\n",
    "    features['V_avg_word_length'] = int(statistics.mean(len(w) for w in word_list))    \n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_text_stat_features():\n",
    "    \n",
    "    feature_sets = [(sub_text_stat_features(row.review), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0e560",
   "metadata": {},
   "source": [
    "**Bigrams**\n",
    "\n",
    "**sub_bigram_features and generate_bigram_features**\n",
    "\n",
    "The sub_bigram_features and generate_bigram_features functions work together to build a feature set for all the reviews. The generate function creates a bi-gram finder and then finds the top bigrams limited to the value provided. It then iterates through each review while calling the sub function. The sub function first finds the bi-grams for the review text and then creates a feature with a value indicating if that bi-gram is present in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fd31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_bigram_features(word_string, bigrams):\n",
    "    \n",
    "    review_bigrams = nltk.bigrams(word_string)\n",
    "    \n",
    "    features = {}\n",
    "    for b in bigrams:\n",
    "        features['B_{}_{}'.format(b[0], b[1])] = (b in review_bigrams)\n",
    "    \n",
    "    return features\n",
    "    \n",
    "def generate_bigram_features(bigram_count):\n",
    "    \n",
    "    finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "    bigrams = finder.nbest(nltk.collocations.BigramAssocMeasures().chi_sq, bigram_count)\n",
    "    \n",
    "    feature_sets = [(sub_bigram_features(row.review, bigrams), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d4f50",
   "metadata": {},
   "source": [
    "**Nots**\n",
    "\n",
    "**is_negation_word**\n",
    "\n",
    "The is_negation_word function determines if the word provided is in the negation word list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3474e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_negation_word(word):\n",
    "    \n",
    "    negation_words = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "    return word in negation_words or word.endswith(\"n't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10653a",
   "metadata": {},
   "source": [
    "**sub_not_features and generate_not_features**\n",
    "\n",
    "The sub_not_features and generate_not_features functions work together to build a feature set for all the reviews. The generate function iterates through each review while calling the sub function. The sub function first tokenizes the review text and then creates a feature with the count of negation words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05645e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_not_features(word_string):\n",
    "    \n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    \n",
    "    features = {}\n",
    "    features['V_not_count'] = sum(1 for w in word_list if is_negation_word(w))    \n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_not_features():\n",
    "    \n",
    "    feature_sets = [(sub_not_features(row.review), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894e3f6",
   "metadata": {},
   "source": [
    "**Negations**\n",
    "\n",
    "**sub_negation_features and generate_negation_features**\n",
    "\n",
    "The sub_negation_features and generate_negation_features functions work together to build a feature set for all the reviews. The generate function first uses the global all word distribution to find the top words limited by the value provided. It then iterates through each review while calling the sub function. The sub function uses the top words to build features by determining if each top word is in the review and preceded by a 'not'. This indicates that the value of the word itself is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_negation_features(word_string, word_features):\n",
    "    \n",
    "    # break up the review text by word\n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    features = {}\n",
    "    \n",
    "    # build all features first\n",
    "    for word in word_features:\n",
    "        features[f'V_not_{word}'] = False\n",
    "        \n",
    "    # unset only the word features matching all conditions\n",
    "    for i in range(0, len(word_features)):\n",
    "        \n",
    "        # get current word feature and determine condition values\n",
    "        word = word_features[i]\n",
    "        pos_exists = (i + 1) < len(word_features)\n",
    "        neg_word = is_negation_word(word)\n",
    "        \n",
    "        #if there's another word and the current word is a negation set the next word as not if its in the review text\n",
    "        if pos_exists and is_negation_word(word):\n",
    "            i += 1\n",
    "            next_word = word_features[i]\n",
    "            next_word_in_list = next_word in word_features\n",
    "            features[f'V_not_{next_word}'] = next_word_in_list\n",
    "            \n",
    "    return features\n",
    "\n",
    "def generate_negation_features(feature_count):\n",
    "    \n",
    "    word_items = word_dist.most_common(feature_count)\n",
    "    word_features = [word for (word,count) in word_items]\n",
    "    \n",
    "    feature_sets = [(sub_negation_features(row.review, word_features), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5853d",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "\n",
    "**tfidf_tokenizer**\n",
    "\n",
    "The tfidf_tokenizer function is used by the TF-IDF Vectorizer to tokenize the review text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b43ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed4df5c",
   "metadata": {},
   "source": [
    "**sub_tfidf_features and generate_tfidf_features**\n",
    "\n",
    "The sub_tfidf_features and generate_tfidf_features functions work together to build a feature set for all the reviews. The generate function first uses the TF-IDF Vectorizer to compute the top terms by term frequency / inverse document frequency value for each review limiting the number of features it tracks to the value provided. It then iterates through each review while calling the sub function. The sub function uses the top tf-idf words and builds features that indicate if the top word is within the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520ee2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_tfidf_features(word_string, tfidf_values):\n",
    "    \n",
    "    word_list = nltk.word_tokenize(word_string)\n",
    "    \n",
    "    features = {}\n",
    "    for word in tfidf_values:\n",
    "        features['TF_{}'.format(word)] = word in word_list\n",
    "        \n",
    "    return features\n",
    "    \n",
    "def generate_tfidf_features(max_features):\n",
    "    \n",
    "    tfidf = TfidfVectorizer(tokenizer=tfidf_tokenizer, stop_words='english', max_features=max_features)\n",
    "    sparse_tfidf_texts = tfidf.fit_transform(reviews_df.review)\n",
    "    tfidf_values = {d:c for (d, c) in zip(tfidf.get_feature_names_out(), tfidf.idf_)}\n",
    "    \n",
    "    feature_sets = [(sub_tfidf_features(row.review, tfidf_values), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141c30",
   "metadata": {},
   "source": [
    "**Vader**\n",
    "\n",
    "**sub_vader_features and generate_vader_features**\n",
    "\n",
    "The sub_vader_features and generate_vader_features functions work together to build a feature set for all the reviews. The generate function first creates the Vader Sentiment Intensity Analyzer and then iterates through each review while passing the analyzer to the sub function. The sub function uses the polarity scores to build features that indicate if the review text is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54030d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_vader_features(word_string, vader_analyzer):\n",
    "\n",
    "    scores = vader_analyzer.polarity_scores(word_string)\n",
    "    agg_score = scores['compound']\n",
    "    features = {}\n",
    "    features['S_vader'] = True if agg_score >= 0.1 else False\n",
    "    \n",
    "    return features\n",
    "    \n",
    "def generate_vader_features():\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    feature_set = [(sub_vader_features(row.review, analyzer), row.sentiment) for (pos,row) in reviews_df.iterrows()]\n",
    "    \n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8f397",
   "metadata": {},
   "source": [
    "### Training Functions\n",
    "\n",
    "**merge_feature_sets**\n",
    "\n",
    "The merge_feature_sets function takes two feature sets and combines the features while retaining the original sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf747db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_feature_sets(set_1, set_2):\n",
    "    \n",
    "    if len(set_1) != len(set_2):\n",
    "        print('the feature sets are of different lengths')\n",
    "        return\n",
    "    \n",
    "    new_set = []\n",
    "    for i in range(0,len(set_1)):\n",
    "        sent = set_1[i][1]\n",
    "        dict_1 = {}\n",
    "        dict_2 = set_1[i][0]\n",
    "        dict_3 = set_2[i][0]\n",
    "        dict_1.update(dict_2)\n",
    "        dict_1.update(dict_3)\n",
    "        tup = (dict_1,sent)\n",
    "        new_set.append(tup)\n",
    "        \n",
    "    return new_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f410c9",
   "metadata": {},
   "source": [
    "**eval_measures**\n",
    "\n",
    "The eval_measures function takes the labels and predictions for a training result and calculates the recall, precision and f1 statistics. It then prints out the statistics into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b9f4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measures(labels, predictions):\n",
    "    \n",
    "    # get a list of labels\n",
    "    distinct_labels = list(set(labels))\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    rec_list = []\n",
    "    prec_list = []\n",
    "    f_list = []\n",
    "    \n",
    "    # for each label, compare gold and predicted lists and compute values\n",
    "    for lab in distinct_labels:\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(labels):\n",
    "            if val == lab and predictions[i] == lab:  TP += 1\n",
    "            if val == lab and predictions[i] != lab:  FN += 1\n",
    "            if val != lab and predictions[i] == lab:  FP += 1\n",
    "            if val != lab and predictions[i] != lab:  TN += 1\n",
    "                \n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        precision = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (recall * precision) / (recall + precision) if (recall + precision) > 0 else 0\n",
    "        \n",
    "        rec_list.append(recall)\n",
    "        prec_list.append(precision)\n",
    "        f_list.append(f1)\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    \n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(distinct_labels):\n",
    "        print(f'{lab}\\t{prec_list[i]:10.3f}\\t{rec_list[i]:10.3f}\\t{f_list[i]:10.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3d1e3",
   "metadata": {},
   "source": [
    "**build_and_test_classifier**\n",
    "\n",
    "The build_and_test_classifier function takes a list of feature_set and the number of cross validation folds to create and then sequentially partitions the feature dataset to build a model using a different holdout set. It then averages the accuracy of all models to get an overall model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b948a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_test_classifier(feature_sets, num_folds):\n",
    "    \n",
    "    # calculate cross validation fold size\n",
    "    fset_length = len(feature_sets)\n",
    "    print(f'feature set length: {fset_length}')\n",
    "    fold_size = int(fset_length / num_folds)\n",
    "    print(f'Fold size: {fold_size}')\n",
    "    print()\n",
    "    \n",
    "    # loop through number of folds and build classifier\n",
    "    acc_results = []\n",
    "    for i in range(num_folds):\n",
    "        \n",
    "        # calculate the position of elements for test and train set\n",
    "        fold_start = i * fold_size\n",
    "        fold_end = fold_start + fold_size\n",
    "        test_set = feature_sets[fold_start:fold_end]\n",
    "        train_set = feature_sets[:fold_start] + feature_sets[fold_end:]\n",
    "        \n",
    "        # train model\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        \n",
    "        # calculate and store accuracy\n",
    "        acc = nltk.classify.accuracy(classifier, test_set)\n",
    "        acc_results.append(acc)\n",
    "        \n",
    "        print(f'Round {i+1} accuracy: {acc}')\n",
    "        \n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for (features, label) in test_set:\n",
    "            labels.append(label)\n",
    "            predictions.append(classifier.classify(features))\n",
    "        \n",
    "        eval_measures(labels, predictions)\n",
    "        \n",
    "        print(classifier.show_most_informative_features(3))\n",
    "        print()\n",
    "            \n",
    "    # print cross validation accuracy\n",
    "    print (f'Mean Accuracy: {((sum(acc_results) / num_folds) * 100):0.2f}%', )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1e60f",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625787ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_df = get_movie_review_text()\n",
    "\n",
    "#print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fc308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "#reviews_df.to_csv(\"reviews.csv\")\n",
    "\n",
    "# load data\n",
    "#reviews_df = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "# Randomize rows\n",
    "reviews_df = reviews_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51146c38",
   "metadata": {},
   "source": [
    "## Review Tokenization and Distribution\n",
    "\n",
    "This section tokenizes all the review texts into a single word list and then builds a frequency distribution that can be used globally for other functions to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecc420e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes all text words and builds a frequency distribution for them\n",
    "tokenized_reviews = [tokenize_and_remove_stopwords(row.review,[]) for (pos,row) in reviews_df.iterrows()]\n",
    "all_words_list = [word for review in tokenized_reviews for word in review]\n",
    "word_dist = nltk.FreqDist(all_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8ff71",
   "metadata": {},
   "source": [
    "## Review Corpus Statistics\n",
    "\n",
    "This sections prints some basic word statistics for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc0eba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 3,188,602\n",
      "Unique Word Count: 104,013\n",
      "Top 50 Words\n",
      "['the', 'movie', 'film', 'one', 'like', 'it', 'this', 'good', 'would', 'even', 'time', 'story', 'really', 'see', 'much', 'well', 'could', 'people', 'get', 'also', 'bad', 'great', 'first', 'made', 'way', 'make', 'movies', 'but', 'think', 'characters', 'and', 'character', 'watch', 'films', 'two', 'many', 'seen', 'never', 'acting', 'little', 'plot', 'best', 'love', 'life', 'show', 'there', 'know', 'in', 'ever', 'better']\n"
     ]
    }
   ],
   "source": [
    "print(f'Word Count: {len(all_words_list):,}')\n",
    "print(f'Unique Word Count: {len(word_dist):,}')\n",
    "print('Top 50 Words')\n",
    "top_ranked_words = word_dist.most_common(50)\n",
    "top_words = [word for (word,count) in top_ranked_words]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b1ecd",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5b068",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Each experiment is iterated across five rounds of cross validation to evaluate the model performance and results. For all model evaluations, the model’s performance in the experiments is evaluated using accuracy, precision, recall and F1 score for both positive (pos) and negative (neg) sentiment classes. Accuracy measures the overall correctness of the model. Precision measures the proportion of true positive instances out of all positive predictions. Recall measures the proportion of true positive instances that were correctly predicted out of all actual positive instances and F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "293477c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of keys for parts-of-speech groups\n",
    "pos_keys = ['conj','determ','noun','verb','adj','adv','particle','marker','num','foreign_word','symbol','interj','to','ex','pos']\n",
    "\n",
    "# training parameters\n",
    "num_folds = 5\n",
    "bow_feature_count = 1500\n",
    "bigram_feature_count = 500\n",
    "negation_feature_count = 2000\n",
    "tfidf_feature_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0f1c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "tic_global = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba42ca",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Experiment 1 is a baseline for experiment 2 and 3. It generates a bag-of-words feature set from all the unique words in the reviews and would includes all words from all parts-of-speech categories. The further experiments will remove varying amounts of text based on parts-of-speech tags to identify if there are any relationships between part-of-speech and the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595908ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 1: test with all pos in bag of words\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8388\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.849\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "\n",
      "Completed in 10.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 1: test with all pos in bag of words')\n",
    "print()\n",
    "bow_1 = generate_bow_features([], bow_feature_count)\n",
    "build_and_test_classifier(bow_1, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4912a0",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, all parts of speech  were included in the bag of words feature set of 25,000 reviews. The performance of each round varies slightly across all five rounds where there is a slight dip in the third round but then improves again in the following round of experimentation. The most indicative features used to distinguish sentiment remain consistent across all rounds where words like \"pointless,\" \"laughable,\" and \"waste\" are strongly associated with negative sentiment. Overall, the mean accuracy across all rounds was 83.14% suggesting the model is reasonably accurate in sentiment classification, but with room for improvement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152db7e",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Experiment 2 iterates through each group of parts-of-speech (ex: all noun types are grouped together) and removes them from the list of words used to determine the bag-of-words. Each iteration will determine what effect that group had on the overall performance. A lower score would indicate that group as having a positive impact which caused a deficit when those words were removed. And conversely, a higher score would indicate that group as having a negative impact on prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b2f756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 2: test with one pos removed at a time\n",
      "\n",
      "build model removing conj pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.825\t     0.826\n",
      "pos\t     0.825\t     0.827\t     0.826\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8368\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.819\t     0.845\t     0.832\n",
      "pos\t     0.854\t     0.829\t     0.842\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8208\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.827\t     0.823\t     0.825\n",
      "pos\t     0.815\t     0.818\t     0.816\n",
      "\n",
      "Most Informative Features\n",
      "             V_redeeming = True              neg : pos    =     10.3 : 1.0\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8408\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.842\t     0.835\t     0.839\n",
      "pos\t     0.839\t     0.847\t     0.843\n",
      "\n",
      "Most Informative Features\n",
      "             V_redeeming = True              neg : pos    =     12.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8394\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.829\t     0.850\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.28%\n",
      "\n",
      "build model removing determ pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8252\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.824\t     0.826\n",
      "pos\t     0.823\t     0.827\t     0.825\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8348\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.840\t     0.830\n",
      "pos\t     0.849\t     0.830\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8174\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.817\t     0.823\n",
      "pos\t     0.806\t     0.818\t     0.812\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8376\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.842\t     0.830\t     0.836\n",
      "pos\t     0.834\t     0.845\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8396\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.830\t     0.849\t     0.840\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.09%\n",
      "\n",
      "build model removing noun pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.845\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.844\t     0.846\t     0.845\n",
      "pos\t     0.846\t     0.844\t     0.845\n",
      "\n",
      "Most Informative Features\n",
      "                  V_4/10 = True              neg : pos    =     37.3 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.0 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     25.5 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8396\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.849\t     0.834\n",
      "pos\t     0.859\t     0.831\t     0.845\n",
      "\n",
      "Most Informative Features\n",
      "                  V_3/10 = True              neg : pos    =     55.8 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     32.3 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     28.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.837\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.838\t     0.842\t     0.840\n",
      "pos\t     0.836\t     0.832\t     0.834\n",
      "\n",
      "Most Informative Features\n",
      "                  V_3/10 = True              neg : pos    =     39.4 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.8 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     23.8 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.856\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.859\t     0.849\t     0.854\n",
      "pos\t     0.853\t     0.863\t     0.858\n",
      "\n",
      "Most Informative Features\n",
      "                  V_7/10 = True              pos : neg    =     35.1 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     30.1 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     24.4 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8528\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.839\t     0.866\t     0.852\n",
      "pos\t     0.867\t     0.840\t     0.853\n",
      "\n",
      "Most Informative Features\n",
      "                  V_4/10 = True              neg : pos    =     31.2 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.8 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     22.2 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 84.61%\n",
      "\n",
      "build model removing verb pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8212\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.831\t     0.815\t     0.823\n",
      "pos\t     0.811\t     0.828\t     0.819\n",
      "\n",
      "Most Informative Features\n",
      "               V_unfunny = True              neg : pos    =     12.8 : 1.0\n",
      "                 V_10/10 = True              pos : neg    =     12.5 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.835\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.829\t     0.834\t     0.832\n",
      "pos\t     0.840\t     0.835\t     0.838\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     14.9 : 1.0\n",
      "               V_unfunny = True              neg : pos    =     13.0 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8116\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.829\t     0.808\t     0.818\n",
      "pos\t     0.794\t     0.816\t     0.805\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     15.2 : 1.0\n",
      "               V_unfunny = True              neg : pos    =     15.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8246\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.810\t     0.825\n",
      "pos\t     0.810\t     0.840\t     0.825\n",
      "\n",
      "Most Informative Features\n",
      "               V_unfunny = True              neg : pos    =     18.7 : 1.0\n",
      "                 V_10/10 = True              pos : neg    =     10.8 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8304\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.835\t     0.831\n",
      "pos\t     0.833\t     0.825\t     0.829\n",
      "\n",
      "Most Informative Features\n",
      "               V_unfunny = True              neg : pos    =     12.0 : 1.0\n",
      "                 V_10/10 = True              pos : neg    =     11.7 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 82.46%\n",
      "\n",
      "build model removing adj pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8022\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.802\t     0.802\t     0.802\n",
      "pos\t     0.802\t     0.802\t     0.802\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     12.5 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 accuracy: 0.812\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.794\t     0.818\t     0.806\n",
      "pos\t     0.829\t     0.806\t     0.817\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     14.9 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "                V_poorly = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.799\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.809\t     0.800\t     0.805\n",
      "pos\t     0.788\t     0.798\t     0.793\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     15.2 : 1.0\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8138\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.812\t     0.810\t     0.811\n",
      "pos\t     0.816\t     0.818\t     0.817\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     10.8 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "           V_wonderfully = True              pos : neg    =      8.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8158\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.799\t     0.830\t     0.814\n",
      "pos\t     0.833\t     0.802\t     0.817\n",
      "\n",
      "Most Informative Features\n",
      "                 V_10/10 = True              pos : neg    =     11.7 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.2 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 80.86%\n",
      "\n",
      "build model removing adv pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8194\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.816\t     0.820\n",
      "pos\t     0.814\t     0.823\t     0.818\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8294\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.812\t     0.837\t     0.824\n",
      "pos\t     0.847\t     0.822\t     0.834\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.815\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.822\t     0.817\t     0.820\n",
      "pos\t     0.808\t     0.813\t     0.810\n",
      "\n",
      "Most Informative Features\n",
      "             V_redeeming = True              neg : pos    =     10.3 : 1.0\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.832\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.837\t     0.824\t     0.830\n",
      "pos\t     0.827\t     0.840\t     0.834\n",
      "\n",
      "Most Informative Features\n",
      "             V_redeeming = True              neg : pos    =     12.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.836\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.824\t     0.847\t     0.836\n",
      "pos\t     0.848\t     0.825\t     0.836\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "             V_redeeming = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 82.64%\n",
      "\n",
      "build model removing particle pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8388\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.849\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing marker pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8388\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.849\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing num pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8254\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.829\t     0.823\t     0.826\n",
      "pos\t     0.822\t     0.828\t     0.825\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8354\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.818\t     0.843\t     0.830\n",
      "pos\t     0.852\t     0.829\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.819\t     0.824\n",
      "pos\t     0.809\t     0.818\t     0.813\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8372\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.841\t     0.830\t     0.835\n",
      "pos\t     0.833\t     0.845\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8396\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.831\t     0.849\t     0.840\n",
      "pos\t     0.848\t     0.831\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.12%\n",
      "\n",
      "build model removing foreign_word pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8388\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.849\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing symbol pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8186\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8388\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.828\t     0.849\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing interj pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8256\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.826\t     0.825\n",
      "pos\t     0.826\t     0.825\t     0.826\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.835\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.821\t     0.840\t     0.830\n",
      "pos\t     0.849\t     0.830\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8184\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.84\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.830\t     0.850\t     0.840\n",
      "pos\t     0.851\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing to pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.826\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.824\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.826\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.835\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.830\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8184\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.824\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4 accuracy: 0.8374\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.839\t     0.831\t     0.835\n",
      "pos\t     0.836\t     0.843\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8402\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.830\t     0.851\t     0.840\n",
      "pos\t     0.851\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "build model removing ex pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.825\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.824\t     0.826\t     0.825\n",
      "pos\t     0.826\t     0.824\t     0.825\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.836\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.821\t     0.842\t     0.831\n",
      "pos\t     0.850\t     0.831\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8182\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.811\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8372\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.838\t     0.832\t     0.835\n",
      "pos\t     0.836\t     0.843\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8386\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.830\t     0.848\t     0.839\n",
      "pos\t     0.847\t     0.830\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.10%\n",
      "\n",
      "build model removing pos pos\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8262\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.827\t     0.826\n",
      "pos\t     0.828\t     0.825\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.5 : 1.0\n",
      "                 V_waste = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8352\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.820\t     0.841\t     0.831\n",
      "pos\t     0.849\t     0.830\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8184\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.825\t     0.821\t     0.823\n",
      "pos\t     0.812\t     0.816\t     0.814\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.2 : 1.0\n",
      "                 V_waste = True              neg : pos    =      9.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.838\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.832\t     0.836\n",
      "pos\t     0.836\t     0.844\t     0.840\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8392\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.829\t     0.850\t     0.839\n",
      "pos\t     0.850\t     0.829\t     0.839\n",
      "\n",
      "Most Informative Features\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      8.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.14%\n",
      "\n",
      "\n",
      "Completed in 212.65 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "bow_2 = {}\n",
    "print('EXPERIMENT 2: test with one pos removed at a time')\n",
    "print()\n",
    "for key in pos_keys:\n",
    "    bow_2[key] = generate_bow_features([key], bow_feature_count)\n",
    "    print(f'build model removing {key} pos')\n",
    "    build_and_test_classifier(bow_2[key], num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c93ebd",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the model’s performance is evaluated with one part of speech removed from the bag of word features during each iteration. The parts of speech removed for each iteration in order are conjugation, determiner, noun, verb, adjective, adverb, particle, marker, numerical, foreign words, symbols, interjection, to, ex (existential there) and pos (posessive endings). \n",
    "\n",
    "When removing conjugation parts of speech, accuracy ranges from 82.08% to 84.08% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment \"pointless,\" \"laughable,\" \"redeeming,\" and \"worst\" are strongly associated with negative sentiment. Overall, the mean accuracy is across all rounds 83.28% suggesting the model is reasonably accurate in sentiment classification when removing conjugation parts of speech but does not show significant signs of improvement.  \n",
    "\n",
    "When removing the determiner parts of speech, accuracy ranges from 81.74% to 83.96% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment \"pointless,\" \"laughable,\" and \"worst\" are strongly associated with negative sentiment. Overall, the mean accuracy is across all rounds 83.09% suggesting the model is reasonably accurate in sentiment classification when removing determiner parts of speech with comparable results to the model’s results when including all parts of speech.\n",
    "\n",
    "When removing noun parts of speech, accuracy ranges from 83.7% to 85.6% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment are \"3/10,\" \"4/10,\" and \"7/10”, which are assumed to be ratings out of ten with the lower scoring ratings corresponding to negative sentiment and the higher rating corresponding to positive sentiment. Overall, the mean accuracy is 84.61%, which is the highest in this experiment, suggesting when removing noun parts of speech is reasonably accurate in sentiment classification but only slightly improved from using all parts of speech. \n",
    "\n",
    "When removing verb parts of speech, accuracy ranges from 81.16% to 83.5% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment are \"unfunny,\" \"pointless,\" and \"10/10\"  where the first two are strongly associated with negative sentiment and the last strongly associated with positive sentiment. The mean accuracy is 82.46% following the same trend as before where the model is reasonably good at classifying sentiment but with no improvement when including all parts of speech.\n",
    "\n",
    "When removing adjective parts of speech, accuracy ranges from 79.9% to 81.58% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment are \"10/10,\" \"pointless,\" and \"waste,\" where the first is associated with positive sentiment and the last two are strongly associated with negative sentiment. Overall, the mean accuracy is 80.86% which is the lowest accuracy in the experiment, still high suggesting a reasonable performance by the model, but slightly lower performance when using all parts of speech.\n",
    "\n",
    "When removing adverb parts of speech, accuracy ranged from 81.5% to 83.6% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment are \"pointless,\" \"laughable,\" and \"redeeming\" which are strongly associated with negative sentiment. With an overall mean accuracy of 82.64% suggests the model is reasonably good at classifying sentiment, but still does not perform as well when including all parts of speech. \n",
    "\n",
    "When removing particle parts of speech, accuracy ranges from 81.86% to 83.88% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment \"pointless,\" “laughable,\" and \"waste\" are strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no improvement when including all parts of speech.\n",
    "\n",
    "When removing marker parts of speech, accuracy ranges from 81.86% to 83.88% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment \"pointless,\" \"laughable,\" and \"waste\" are strongly associated with negative sentiment. The mean accuracy is 83.14% again suggesting the model is reasonable at classifying sentiment, but with no improvement when including all parts of speech.\n",
    "\n",
    "When removing numerical parts of speech, accuracy ranges from 81.86% to 83.96% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.12% showing the model is reasonable at classifying sentiment, but with no improvement when including all parts of speech.\n",
    "\n",
    "When removing foreign parts of speech, accuracy ranged from 81.86% to 83.88% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no change when including all parts of speech.\n",
    "\n",
    "When removing symbol parts of speech, accuracy ranges from 81.86% to 83.88% across the five rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no improvement when including all parts of speech.\n",
    "\t\n",
    "When removing interjection parts of speech, accuracy ranges from 81.84% to 84.0% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no change when including all parts of speech.\n",
    "\n",
    "When removing to parts of speech, accuracy ranges from 81.84% to 84.02% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no change when including all parts of speech.\n",
    "\n",
    "When removing ex parts of speech, accuracy ranges from 81.82% to 83.86% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy but vary slightly across rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no change when including all parts of speech.\n",
    "\n",
    "When removing pos parts of speech, accuracy ranges from 81.84% to 83.92% across the five rounds. The most indicative features used to distinguish sentiment were again “pointless,\" \"laughable,\" and \"waste\" which were strongly associated with negative sentiment. The mean accuracy is 83.14% showing the model is reasonable at classifying sentiment, but with no improvement when including all parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1da8d",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "\n",
    "Experiment 3 removes all parts-of-speech before generating the bag-of-words features and what is left is only what it could not define. This is similar to the baseline from experiment 1 but on the other extreme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e2d5ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 3: test all pos removed\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.5964\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.552\t     0.606\t     0.578\n",
      "pos\t     0.641\t     0.589\t     0.614\n",
      "\n",
      "Most Informative Features\n",
      "                V_'zombi = True              neg : pos    =      7.7 : 1.0\n",
      "            V_winchester = True              pos : neg    =      7.3 : 1.0\n",
      "                 V_hanzo = True              pos : neg    =      6.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.5886\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.522\t     0.593\t     0.556\n",
      "pos\t     0.653\t     0.585\t     0.617\n",
      "\n",
      "Most Informative Features\n",
      "            V_winchester = True              pos : neg    =     16.5 : 1.0\n",
      "                V_'zombi = True              neg : pos    =      6.9 : 1.0\n",
      "                 V_hanzo = True              pos : neg    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.593\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.543\t     0.616\t     0.577\n",
      "pos\t     0.645\t     0.574\t     0.608\n",
      "\n",
      "Most Informative Features\n",
      "                V_'zombi = True              neg : pos    =      9.1 : 1.0\n",
      "            V_winchester = True              pos : neg    =      8.1 : 1.0\n",
      "                 V_hanzo = True              pos : neg    =      4.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.5958\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.533\t     0.599\t     0.564\n",
      "pos\t     0.656\t     0.593\t     0.623\n",
      "\n",
      "Most Informative Features\n",
      "            V_winchester = True              pos : neg    =      7.1 : 1.0\n",
      "                V_'zombi = True              neg : pos    =      6.3 : 1.0\n",
      "                 V_hanzo = True              pos : neg    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.6026\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.527\t     0.627\t     0.573\n",
      "pos\t     0.680\t     0.584\t     0.628\n",
      "\n",
      "Most Informative Features\n",
      "            V_winchester = True              pos : neg    =      5.5 : 1.0\n",
      "                 V_batty = True              pos : neg    =      3.4 : 1.0\n",
      "                 V_horny = True              neg : pos    =      3.1 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 59.53%\n",
      "\n",
      "\n",
      "Completed in 8.69 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 3: test all pos removed')\n",
    "print()\n",
    "bow_3 = generate_bow_features(pos_keys, bow_feature_count)\n",
    "build_and_test_classifier(bow_3, num_folds)\n",
    "print() \n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ad8ee",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, all tagged parts of speech were removed from the bag of words feature set. The accuracy ranges from 58.86% to 60.26% dropping significantly from the previous experimental results. The most indicative features used to distinguish sentiment were \"winchester,\" \"hanzo,\" and \"zombi\"  where the first two are associated with positive sentiment and the last associated with negative sentiment. The mean accuracy is 59.53% significantly lower than all parts of speech included in the bag of word features, and lower than removing just one part of speech tag at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690988a",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "Experiment 4 generates features by using counts of parts of speech within a review. The goal is to determine if the volume of parts-of-speech tags are any indication of a positive or negative review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e805db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 4: test pos counts\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.542\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.644\t     0.535\t     0.584\n",
      "pos\t     0.440\t     0.553\t     0.490\n",
      "\n",
      "Most Informative Features\n",
      "            V_verb_count = 173               neg : pos    =      6.3 : 1.0\n",
      "            V_verb_count = 148               neg : pos    =      5.7 : 1.0\n",
      "          V_determ_count = 120               neg : pos    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.5448\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.657\t     0.530\t     0.587\n",
      "pos\t     0.436\t     0.567\t     0.493\n",
      "\n",
      "Most Informative Features\n",
      "            V_verb_count = 148               neg : pos    =      6.9 : 1.0\n",
      "             V_adv_count = 81                neg : pos    =      6.3 : 1.0\n",
      "            V_noun_count = 205               pos : neg    =      5.4 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.5536\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.660\t     0.553\t     0.602\n",
      "pos\t     0.442\t     0.554\t     0.492\n",
      "\n",
      "Most Informative Features\n",
      "            V_verb_count = 156               pos : neg    =      7.6 : 1.0\n",
      "          V_determ_count = 120               neg : pos    =      5.7 : 1.0\n",
      "            V_verb_count = 148               neg : pos    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.5548\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.647\t     0.539\t     0.588\n",
      "pos\t     0.466\t     0.578\t     0.516\n",
      "\n",
      "Most Informative Features\n",
      "            V_noun_count = 248               pos : neg    =      6.4 : 1.0\n",
      "          V_determ_count = 120               neg : pos    =      6.3 : 1.0\n",
      "            V_verb_count = 170               pos : neg    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.542\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.634\t     0.540\t     0.583\n",
      "pos\t     0.448\t     0.545\t     0.492\n",
      "\n",
      "Most Informative Features\n",
      "            V_verb_count = 170               pos : neg    =      5.6 : 1.0\n",
      "          V_determ_count = 120               neg : pos    =      5.0 : 1.0\n",
      "            V_noun_count = 230               neg : pos    =      5.0 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 54.74%\n",
      "\n",
      "\n",
      "Completed in 6.33 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 4: test pos counts')\n",
    "print()\n",
    "poscounts = generate_pos_features()\n",
    "build_and_test_classifier(poscounts, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153194e",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the features set was composed of counts of tagged parts of speech including conjugations, determiner parts of speech, nouns, verbs, adjectives, adverbs, particles, markers, numerical parts of speech, foreign words, symbol parts of speech, and interjection parts of speech. The accuracy ranges from 52.4 % to 55.48% across the five rounds.  The precision, recall, and F1 scores are similar with respect to accuracy varying slightly across rounds, but the F1 score for classifying negative sentiment was slightly higher than classifying positive sentiment. This suggests the model is more precise and has a greater recall classifying negative sentiment rather than positive sentiment. The most indicative features used to distinguish sentiment were \"verb_count\" associated with positive sentiment,  \"determ_count\" associated with negative sentiment, “noun_count” associated with positive sentiment. This suggests the model finds reviews with high number of verbs and noun parts of speech to be associated with positive sentiment and higher determiner parts of speech to be associated with negative sentiment. The mean accuracy was accuracy across all rounds is approximately 54.74%, showing that removing noun parts of speech on the bag of words feature set still has a better performance when classifying sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c26535",
   "metadata": {},
   "source": [
    "## Experiment 5\n",
    "\n",
    "Experiment 5 generates features by counting the number of capital letters, measuring the length of the review and average word length. There may be a relationship between how lengthy or concise a review is, the overuse of caps or even the word complexity (more syllables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e1de75e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 5: text statistics features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.5138\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.532\t     0.513\t     0.522\n",
      "pos\t     0.496\t     0.515\t     0.505\n",
      "\n",
      "Most Informative Features\n",
      "         V_review_length = 241               pos : neg    =      6.3 : 1.0\n",
      "         V_review_length = 930               pos : neg    =      6.3 : 1.0\n",
      "         V_review_length = 1081              neg : pos    =      6.3 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.519\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.529\t     0.511\t     0.520\n",
      "pos\t     0.509\t     0.527\t     0.518\n",
      "\n",
      "Most Informative Features\n",
      "         V_review_length = 855               neg : pos    =      7.6 : 1.0\n",
      "         V_review_length = 1363              pos : neg    =      7.0 : 1.0\n",
      "         V_review_length = 1104              neg : pos    =      7.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.517\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.531\t     0.527\t     0.529\n",
      "pos\t     0.502\t     0.506\t     0.504\n",
      "\n",
      "Most Informative Features\n",
      "         V_review_length = 1043              pos : neg    =      8.9 : 1.0\n",
      "         V_review_length = 379               neg : pos    =      7.1 : 1.0\n",
      "         V_review_length = 1208              neg : pos    =      6.4 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.5212\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.559\t     0.511\t     0.534\n",
      "pos\t     0.485\t     0.533\t     0.508\n",
      "\n",
      "Most Informative Features\n",
      "         V_review_length = 1043              pos : neg    =      6.4 : 1.0\n",
      "         V_review_length = 266               pos : neg    =      5.7 : 1.0\n",
      "         V_review_length = 1081              neg : pos    =      5.6 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.5276\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.556\t     0.531\t     0.543\n",
      "pos\t     0.498\t     0.523\t     0.511\n",
      "\n",
      "Most Informative Features\n",
      "         V_review_length = 1457              pos : neg    =      6.3 : 1.0\n",
      "             V_cap_count = 135               pos : neg    =      6.3 : 1.0\n",
      "         V_review_length = 1241              neg : pos    =      5.7 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 51.97%\n",
      "\n",
      "\n",
      "Completed in 1.19 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 5: text statistics features')\n",
    "print()\n",
    "ts = generate_text_stat_features()\n",
    "build_and_test_classifier(ts, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7c92f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment the feature statistics were used to generate the feature set. These statistics are composed of capitalized word count review length and an average word length. The accuracy ranges from 51.38% to 52.76% across the five rounds. Again, this range drops from the initial set of experiments but the precision, recall, and F1 scores are similar with respect to accuracy. Again, the F1 score for classifying negative sentiment was slightly higher than classifying positive sentiment suggesting the model is more precise and has a greater recall classifying negative sentiment rather than positive sentiment. The most indicative features used to distinguish sentiment was “review_length” followed by “cap_count” where each is not specifically associated with positive or negative sentiment, but the model identified them as determinative when classifying the reviews. The overall accuracy of all rounds is 51.87% decreasing from the previous experiment, and only slightly better than a coin flip. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c595d4",
   "metadata": {},
   "source": [
    "## Experiment 6\n",
    "\n",
    "Experiment 6 uses bi-grams collected from the review corpus to generate features from. If there are common two word phrases that indicate if the review is negative or positive this will identify what those are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53478c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 6: bi-gram features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.4998\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.500\t     0.666\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "        B_'achcha_pitaji = False             neg : pos    =      1.0 : 1.0\n",
      "B_'acts_operetta-structure = False             neg : pos    =      1.0 : 1.0\n",
      "       B_'airs_12th-14th = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.4924\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.492\t     0.660\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "        B_'achcha_pitaji = False             neg : pos    =      1.0 : 1.0\n",
      "B_'acts_operetta-structure = False             neg : pos    =      1.0 : 1.0\n",
      "       B_'airs_12th-14th = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.4888\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.000\t     0.000\t     0.000\n",
      "pos\t     1.000\t     0.489\t     0.657\n",
      "\n",
      "Most Informative Features\n",
      "        B_'achcha_pitaji = False             neg : pos    =      1.0 : 1.0\n",
      "B_'acts_operetta-structure = False             neg : pos    =      1.0 : 1.0\n",
      "       B_'airs_12th-14th = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.491\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.491\t     0.659\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "        B_'achcha_pitaji = False             neg : pos    =      1.0 : 1.0\n",
      "B_'acts_operetta-structure = False             neg : pos    =      1.0 : 1.0\n",
      "       B_'airs_12th-14th = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.4944\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.000\t     0.000\t     0.000\n",
      "pos\t     1.000\t     0.494\t     0.662\n",
      "\n",
      "Most Informative Features\n",
      "        B_'achcha_pitaji = False             neg : pos    =      1.0 : 1.0\n",
      "B_'acts_operetta-structure = False             neg : pos    =      1.0 : 1.0\n",
      "       B_'airs_12th-14th = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 49.33%\n",
      "\n",
      "\n",
      "Completed in 3.80 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 6: bi-gram features')\n",
    "print()\n",
    "bigrams = generate_bigram_features(bigram_feature_count)\n",
    "build_and_test_classifier(bigrams, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee2809",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, a bigram finder is used to find the top bigrams iterated across all reviews. The bigrams are then used as features to classify within the model. The accuracy ranges from 48.88% to 49.98% across the five rounds. In an unusual case, one class of sentiment had perfect precision and the other had a precision of 0.0 where the class depended on the round of the experiment. Where the model precision was perfect, the recall hovered around 59% and the F1 score was higher than the accuracy score found usually around 66%. When the precision of one of the sentiment classes was 0.0 the recall and the F1 score were also 0.0. This could be because the bigrams were comprised of only negated bi-gram features or positive bigram features. The most indicative features were found to be these negated bigram features where the model determined them to have a strong association with negative sentiment implying the model with this feature set is biased towards predicting instances at negative. It’s important to remember, the mean accuracy is 49.33%, indicating that the model's performance is not much better than random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf6eb0",
   "metadata": {},
   "source": [
    "## Experiment 7\n",
    "\n",
    "Experiment 7 generate a feature that counts the number of 'not' words. The outcome should indicate if this statistic is a good barometer of sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeef9c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 7: not features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.5794\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.617\t     0.574\t     0.595\n",
      "pos\t     0.542\t     0.586\t     0.563\n",
      "\n",
      "Most Informative Features\n",
      "             V_not_count = 19                neg : pos    =      3.5 : 1.0\n",
      "             V_not_count = 20                neg : pos    =      3.4 : 1.0\n",
      "             V_not_count = 13                neg : pos    =      2.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.5774\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.600\t     0.567\t     0.583\n",
      "pos\t     0.555\t     0.589\t     0.571\n",
      "\n",
      "Most Informative Features\n",
      "             V_not_count = 17                neg : pos    =      3.6 : 1.0\n",
      "             V_not_count = 13                neg : pos    =      2.7 : 1.0\n",
      "             V_not_count = 19                neg : pos    =      2.4 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.5826\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.608\t     0.589\t     0.598\n",
      "pos\t     0.556\t     0.576\t     0.565\n",
      "\n",
      "Most Informative Features\n",
      "             V_not_count = 23                neg : pos    =      3.7 : 1.0\n",
      "             V_not_count = 19                neg : pos    =      2.6 : 1.0\n",
      "             V_not_count = 17                neg : pos    =      2.5 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.586\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.612\t     0.573\t     0.592\n",
      "pos\t     0.561\t     0.600\t     0.580\n",
      "\n",
      "Most Informative Features\n",
      "             V_not_count = 20                neg : pos    =      3.4 : 1.0\n",
      "             V_not_count = 24                neg : pos    =      3.0 : 1.0\n",
      "             V_not_count = 26                neg : pos    =      3.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.5912\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.623\t     0.591\t     0.606\n",
      "pos\t     0.559\t     0.592\t     0.575\n",
      "\n",
      "Most Informative Features\n",
      "             V_not_count = 20                neg : pos    =      3.0 : 1.0\n",
      "             V_not_count = 13                neg : pos    =      2.6 : 1.0\n",
      "             V_not_count = 15                neg : pos    =      2.4 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 58.33%\n",
      "\n",
      "\n",
      "Completed in 1.15 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 7: not features')\n",
    "print()\n",
    "nots = generate_not_features()\n",
    "build_and_test_classifier(nots, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c2b4f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the number instances of the word “not” were counted in each review across both sets of reviews to generate a feature set used to classify sentiment. The accuracy ranges from 57.74% to 58.33% across the five rounds. The precision, recall, and F1 scores are similar with respect to accuracy varying slightly across rounds, but the F1 score for classifying negative sentiment was slightly higher than classifying positive sentiment. This suggests the model is more precise and has a greater recall classifying negative sentiment rather than positive sentiment. This logically follows when creating a list of features comprised of the number of times “not”, a negation word, is used across reviews. The most indicative features were the number of “not_counts” found within the reviews where they were all associated with negative sentiment. The overall mean accuracy 58.33% was higher than the previous experiment, but still not as high as the initial experiments removing noun parts of speech and the bag of words feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477a9e7",
   "metadata": {},
   "source": [
    "## Experiment 8\n",
    "\n",
    "Experiment 8 is similar to experiment 7 but instead of counting the not words it indicates which words were contradicted. Not all words were indicated. This only used the most common words by frequency to determine if each review contained a contradicted version of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58a62442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 8: negation features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.4998\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.500\t     0.666\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "                 V_not_& = False             neg : pos    =      1.0 : 1.0\n",
      "              V_not_'the = False             neg : pos    =      1.0 : 1.0\n",
      "             V_not_..... = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.4924\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.492\t     0.660\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "                 V_not_& = False             neg : pos    =      1.0 : 1.0\n",
      "              V_not_'the = False             neg : pos    =      1.0 : 1.0\n",
      "             V_not_..... = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.4888\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.000\t     0.000\t     0.000\n",
      "pos\t     1.000\t     0.489\t     0.657\n",
      "\n",
      "Most Informative Features\n",
      "                 V_not_& = False             neg : pos    =      1.0 : 1.0\n",
      "              V_not_'the = False             neg : pos    =      1.0 : 1.0\n",
      "             V_not_..... = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.491\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     1.000\t     0.491\t     0.659\n",
      "pos\t     0.000\t     0.000\t     0.000\n",
      "\n",
      "Most Informative Features\n",
      "                 V_not_& = False             neg : pos    =      1.0 : 1.0\n",
      "              V_not_'the = False             neg : pos    =      1.0 : 1.0\n",
      "             V_not_..... = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.4944\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.000\t     0.000\t     0.000\n",
      "pos\t     1.000\t     0.494\t     0.662\n",
      "\n",
      "Most Informative Features\n",
      "                 V_not_& = False             neg : pos    =      1.0 : 1.0\n",
      "              V_not_'the = False             neg : pos    =      1.0 : 1.0\n",
      "             V_not_..... = False             neg : pos    =      1.0 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 49.33%\n",
      "\n",
      "\n",
      "Completed in 14.27 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 8: negation features')\n",
    "print()\n",
    "negate = generate_negation_features(negation_feature_count)\n",
    "build_and_test_classifier(negate, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c543f",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the feature set used to classify sentiment is composed of top words preceded by the word “not” across the global word distribution set. The accuracy ranges from 48.88% to 49.98% across all five rounds. Again, the behavior of the precision, recall and F1 score results are similar to the bi-gram features where only one class of sentiment had perfect precision and the other had a precision of 0.0. Where the model precision was perfect, the recall hovered around 49%, and the F1 score was about 66%. When the precision of one of the sentiment classes was 0.0 the recall and the F1 score were also 0.0. The same phenomenon that occurred when using the bigram feature set suggesting sets could be composed of wholly negated pairs of top word or perhaps double negated words when the positive class precision is 100% accurate. There are no indicative features that include double negation, but they do include features like “not_&” and “not_the” which are associated with negative sentiment. The mean accuracy was found to be 49.33%, the same when using the bigram feature set and not improving from previous experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bf6a0",
   "metadata": {},
   "source": [
    "## Experiment 9\n",
    "\n",
    "Experiment 9 is similar to the bag-of-words features in experiments 1-3. The TF-IDF uses frequency number countered by overuse. If a word is used a lot it is probably a good word to compare against but if it's used too much then it loses it's meaning and should be penalized. This test should determine how this compares to the bag-of-words tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baa211ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 9: tf-idf features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8246\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.799\t     0.842\t     0.820\n",
      "pos\t     0.850\t     0.809\t     0.829\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     14.1 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     10.5 : 1.0\n",
      "                TF_awful = True              neg : pos    =      8.5 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8214\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.782\t     0.844\t     0.812\n",
      "pos\t     0.859\t     0.803\t     0.830\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     12.3 : 1.0\n",
      "               TF_poorly = True              neg : pos    =      9.1 : 1.0\n",
      "                TF_worst = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8086\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.787\t     0.830\t     0.808\n",
      "pos\t     0.831\t     0.789\t     0.809\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     15.9 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     10.4 : 1.0\n",
      "                TF_worst = True              neg : pos    =      8.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8282\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.805\t     0.838\t     0.822\n",
      "pos\t     0.850\t     0.819\t     0.834\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     12.2 : 1.0\n",
      "            TF_pointless = True              neg : pos    =      9.7 : 1.0\n",
      "                TF_worst = True              neg : pos    =      9.2 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8268\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.789\t     0.857\t     0.822\n",
      "pos\t     0.866\t     0.800\t     0.832\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     11.7 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "                TF_worst = True              neg : pos    =      9.2 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 82.19%\n",
      "\n",
      "\n",
      "Completed in 10.57 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 9: tf-idf features')\n",
    "print()\n",
    "tfidf = generate_tfidf_features(tfidf_feature_count)\n",
    "build_and_test_classifier(tfidf, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d7bc4",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the TF_IDF vectorizer is used to compute the top terms while considering their frequency across the entire corpora. This feature set is then used to classify sentiment where the accuracy ranges from 80.86% to 82.68%.  The precision, recall, and F1 scores are similar with respect to accuracy with the F1 scores for positive sentiment classification being slightly higher than the negative sentiment classification. This suggests the slightest favorability when classifying positive sentiment using the model when using this feature set. The indicative features used to distinguish sentiment were \"waste,\" \"pointless,\" and \"worst\" which are strongly associated with negative sentiment. The mean accuracy across all rounds was 82.14% which is much better than the previous experiments but still slightly worse than the first experiment using all parts of speech and the bag of words feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944425b",
   "metadata": {},
   "source": [
    "## Experiment 10\n",
    "\n",
    "Experiment 10 uses a 3rd party sentiment library to generate features with a sentiment value. In testing it was never completely accurate but may provide some improved accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec495f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 10: vader sentiment features\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.7018\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.547\t     0.792\t     0.647\n",
      "pos\t     0.856\t     0.654\t     0.742\n",
      "\n",
      "Most Informative Features\n",
      "                 S_vader = False             neg : pos    =      3.6 : 1.0\n",
      "                 S_vader = True              pos : neg    =      1.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.7054\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.544\t     0.793\t     0.645\n",
      "pos\t     0.862\t     0.661\t     0.748\n",
      "\n",
      "Most Informative Features\n",
      "                 S_vader = False             neg : pos    =      3.6 : 1.0\n",
      "                 S_vader = True              pos : neg    =      1.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.6938\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.554\t     0.783\t     0.649\n",
      "pos\t     0.840\t     0.643\t     0.728\n",
      "\n",
      "Most Informative Features\n",
      "                 S_vader = False             neg : pos    =      3.7 : 1.0\n",
      "                 S_vader = True              pos : neg    =      1.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.7044\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.553\t     0.781\t     0.647\n",
      "pos\t     0.851\t     0.663\t     0.746\n",
      "\n",
      "Most Informative Features\n",
      "                 S_vader = False             neg : pos    =      3.6 : 1.0\n",
      "                 S_vader = True              pos : neg    =      1.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.6908\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.544\t     0.778\t     0.640\n",
      "pos\t     0.841\t     0.643\t     0.729\n",
      "\n",
      "Most Informative Features\n",
      "                 S_vader = False             neg : pos    =      3.7 : 1.0\n",
      "                 S_vader = True              pos : neg    =      1.9 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 69.92%\n",
      "\n",
      "\n",
      "Completed in 2.52 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 10: vader sentiment features')\n",
    "print()\n",
    "vader = generate_vader_features()\n",
    "build_and_test_classifier(vader, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "toc = time.perf_counter()\n",
    "print(f\"Completed in {((toc-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba8f98",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, the feature set used to classify across was generated using Vader Sentiment Intensity Analyzer. Everything else being equal, the accuracy ranges from 69.08% to 70.54%. The precision, recall, and F1 scores are similar with respect to accuracy but there were slight differences between classes. For example, for the negative class, precision ranges from approximately 0.544 to 0.554, recall ranges from approximately 0.778 to 0.793, and F1-score ranges from approximately 0.640 to 0.649. For the positive class, precision ranges were higher from approximately 0.840 to 0.862, the recall scores were lower from approximately 0.643 to 0.663, and the F1 scores were thus higher from approximately 0.728 to 0.748. This could indicate a slight favoritism towards classifying positive sentiment when using the model with this feature set. The most informative features where sentiment analyzed features were either present or not. Interestingly, the absence of sentiment (S_vader = False) is more indicative of the negative class, while the presence of sentiment (S_vader = True) is more indicative of the positive class. The mean accuracy across all rounds was found to be 69.92%, showing a moderate performance but not as good as performance from Experiment 1 or Experiment 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d64cb",
   "metadata": {},
   "source": [
    "## Experiment 11\n",
    "\n",
    "Experiment 11 combines features from previous tests to create a set that has a higher score than each individual feature set. In this case the features from Vader, TF-IDF and Bag-of-Words (all words) were used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04a11a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 11: combined features (1)\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.8344\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.823\t     0.842\t     0.832\n",
      "pos\t     0.846\t     0.827\t     0.836\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     14.1 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     10.5 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8408\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.812\t     0.857\t     0.834\n",
      "pos\t     0.868\t     0.827\t     0.847\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     12.3 : 1.0\n",
      "             V_laughable = True              neg : pos    =      9.7 : 1.0\n",
      "                 V_worst = True              neg : pos    =      9.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.829\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.824\t     0.839\t     0.831\n",
      "pos\t     0.834\t     0.819\t     0.827\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     15.9 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     10.4 : 1.0\n",
      "             V_pointless = True              neg : pos    =     10.1 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8472\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.840\t     0.847\t     0.844\n",
      "pos\t     0.854\t     0.847\t     0.850\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     12.2 : 1.0\n",
      "            TF_pointless = True              neg : pos    =      9.7 : 1.0\n",
      "             V_pointless = True              neg : pos    =      9.6 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8444\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.819\t     0.866\t     0.842\n",
      "pos\t     0.870\t     0.825\t     0.847\n",
      "\n",
      "Most Informative Features\n",
      "                TF_waste = True              neg : pos    =     11.7 : 1.0\n",
      "             V_pointless = True              neg : pos    =     11.5 : 1.0\n",
      "            TF_pointless = True              neg : pos    =     11.3 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 83.92%\n",
      "\n",
      "\n",
      "Completed in 17.04 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 11: combined features (1)')\n",
    "print()\n",
    "new_1 = merge_feature_sets(vader, tfidf)\n",
    "new_2 = merge_feature_sets(new_1, bow_1)\n",
    "build_and_test_classifier(new_2, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec8379",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, multiple feature sets were combined including the Vader feature set, the TF_IDF feature set and the bag of words feature set including all parts of speech. The accuracy ranged from 82.9% to 84.72% across all five rounds. We see similar results to the previous experiment in terms of the precision, recall and F1 scores to be slightly higher in the positive class than the negative class but with less of a difference between the two. The negative class precision ranges from approximately 0.812 to 0.840, recall ranges from approximately 0.839 to 0.866, and F1-score ranges from approximately 0.831 to 0.844. The positive class precision ranges from approximately 0.854 to 0.870, recall ranges from approximately 0.819 to 0.827, and F1-score ranges from approximately 0.836 to 0.850. The most informative features were a combination of features from the merged list of features. For example, TF_DIF vectorized features like \"TF_waste\" and sentiment features like \"V_pointless\" were found to be associated with negative sentiment. The mean accuracy across all rounds was found to be 83.92% making this model just slightly outperform the performance seen in Experiment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ca3c8",
   "metadata": {},
   "source": [
    "## Experiment 12\n",
    "\n",
    "Experiment 12 is similar to experiment 11. This will combine the features from Vader, TF-IDF and Bag-of-Words (with nouns removed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "502f84b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 12: combined features (2)\n",
      "\n",
      "feature set length: 25000\n",
      "Fold size: 5000\n",
      "\n",
      "Round 1 accuracy: 0.85\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.834\t     0.861\t     0.848\n",
      "pos\t     0.866\t     0.839\t     0.852\n",
      "\n",
      "Most Informative Features\n",
      "                  V_4/10 = True              neg : pos    =     37.3 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.0 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     25.5 : 1.0\n",
      "None\n",
      "\n",
      "Round 2 accuracy: 0.8462\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.811\t     0.868\t     0.838\n",
      "pos\t     0.881\t     0.827\t     0.853\n",
      "\n",
      "Most Informative Features\n",
      "                  V_3/10 = True              neg : pos    =     55.8 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     32.3 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     28.9 : 1.0\n",
      "None\n",
      "\n",
      "Round 3 accuracy: 0.8402\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.833\t     0.852\t     0.842\n",
      "pos\t     0.848\t     0.829\t     0.838\n",
      "\n",
      "Most Informative Features\n",
      "                  V_3/10 = True              neg : pos    =     39.4 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.8 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     23.8 : 1.0\n",
      "None\n",
      "\n",
      "Round 4 accuracy: 0.8616\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.851\t     0.865\t     0.858\n",
      "pos\t     0.872\t     0.858\t     0.865\n",
      "\n",
      "Most Informative Features\n",
      "                  V_7/10 = True              pos : neg    =     35.1 : 1.0\n",
      "                  V_4/10 = True              neg : pos    =     30.1 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     24.4 : 1.0\n",
      "None\n",
      "\n",
      "Round 5 accuracy: 0.8554\n",
      "\tPrecision\tRecall\t\tF1\n",
      "neg\t     0.830\t     0.877\t     0.853\n",
      "pos\t     0.881\t     0.835\t     0.858\n",
      "\n",
      "Most Informative Features\n",
      "                  V_4/10 = True              neg : pos    =     31.2 : 1.0\n",
      "                  V_7/10 = True              pos : neg    =     27.8 : 1.0\n",
      "                  V_3/10 = True              neg : pos    =     22.2 : 1.0\n",
      "None\n",
      "\n",
      "Mean Accuracy: 85.07%\n",
      "\n",
      "\n",
      "Completed in 22.78 minutes\n"
     ]
    }
   ],
   "source": [
    "# restart timer\n",
    "tic = time.perf_counter()\n",
    "\n",
    "print('EXPERIMENT 12: combined features (2)')\n",
    "print()\n",
    "new_1 = merge_feature_sets(vader, tfidf)\n",
    "new_2 = merge_feature_sets(new_1, bow_2['noun'])\n",
    "build_and_test_classifier(new_2, num_folds)\n",
    "print()\n",
    "\n",
    "# check end time\n",
    "print(f\"Completed in {((time.perf_counter()-tic)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32bcc9",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "In this experiment, multiple feature sets were combined including the Vader feature set, the TF_IDF feature set and the bag of words feature set with the nouns removed from the parts of speech included. This was done to build off the promising results of the previous experiment and using results from experiment 2 where nouns were removed from the bag of word features improved the performance. The accuracy ranged from 84.02% to 85.54% across all five rounds. The same pattern emerges in regard to the previous experiment’s precision, recall and F1 scores being slightly higher in the positive class than the negative class. The negative class had precision ranges from approximately 0.811 to 0.851, recall ranges from approximately 0.852 to 0.877, and F1-score ranges from approximately 0.838 to 0.858, and the positive class had precision ranges from approximately 0.866 to 0.881, recall ranges from approximately 0.827 to 0.858, and F1-score ranges from approximately 0.852 to 0.865. This indicates a slight favoritism towards the positive class sentiment in terms of classifying the reviews, but again very slight. The most informative features appear to be numerical ratings like in the experiment where nouns were removed previously. The same features appear \"3/10,\" \"4/10,\" and \"7/10”, with the lower scoring ratings corresponding to negative sentiment and the higher rating corresponding to positive sentiment. The mean accuracy across all rounds is found to be 85.07%, the most promising results from the model across all experiments and iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6508e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to finish in 318.96 minutes\n"
     ]
    }
   ],
   "source": [
    "# Stop timer\n",
    "print(f\"Start to finish in {((time.perf_counter()-tic_global)/60):0.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e4bcd",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Through all experiments a prediction accuracy of 85% was the highest achieved. It was through experiment 12 which combined the features sets with the overall highest accuracy individually. These features were Vader, TF-IDF and Bag-of-Words (with nouns removed). The improvement was modest of 1-2 points from using the feature sets as standalone but was relatively high.\n",
    "\n",
    "With more time other avenues could've been attempted to mix feature logic or spend more time within the review text to identify other aspects of the corpus that indicated movie sentiment with better accuracy. Overall the results were respectable but we wished we could've designed features that reached at least to 90% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
